%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[12pt, journal]{IEEEtran}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{etoolbox}
\usepackage{hyperref}
\AtBeginEnvironment{minted}{\fontsize{10}{10}\selectfont}
		
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley and Jeff Goldberg.
% This package may be useful when used in conjunction with IEEEtran.cls'
% captionsoff option. Some IEEE journals/societies require that submissions
% have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.3.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% For subfigure.sty:
% \let\MYorigsubfigure\subfigure
% \renewcommand{\subfigure}[2][\relax]{\MYorigsubfigure[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat/subfig command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Generating NFL Game Headlines from Box Score Statistics}

%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{\textbf{Parker Greene}, \textbf{Itamar Belson}, and \textbf{John Clarke}\\\{pwgreene, ibelson, jaclarke\}@mit.edu\\}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~6, No.~1, January~2007}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% IEEEtran, journal, \LaTeX, paper, template.
% \end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\textbf{Abstract }\textit{In this paper, we introduce a new method by which to extrapolate key game statistics from a complete, purely numerical box score in order to produce a one-sentence summary of the game. Through identifying the particular game figures of greatest importance, the system is able to generate informative headlines that effectively highlight the most significant elements of the particular game. We describe a system that: (a) successfully utilizes a neural network to identify the significant elements of information necessary to summarize the complete set of data; (b) uses the important data components to generate complete, comprehensive headlines. In this paper we explore the application of the methods to football games played in the National Football League (NFL), but the same approach may be applied to other sports or to a number of other applications. }

\section{Introduction}
Sports analytics is an emerging field that seeks to incorporate principles of big data analytics into the world of sports. Data in this arena is extensive, and largely publicly accessible, due in part to the economic appeal of the sports industry as well as the popular public interest in sports. Yet, although considerable work has been completed in the field, much of the work centers on game prediction and individual player evaluation as these applications provide tangible economic benefits to the team franchises. While work for these applications is ongoing, there has been significantly less work in the process of analyzing a complete set of statistics to extrapolate the significant components within a particular game.

Whether a key player's performance or the overall struggle in the contest, concluding such information currently requires either watching the complete game or a comprehensive understanding of and ability to read thorough a publicly available box score. A popular alternative to the two is to read game summaries written by sports professionals, whose expertise in the matter enables them to recap a complete game in a condensed form that is understandable to any reader.

This paper explores the potential of machine learning and natural language processing approaches and techniques to extract the important game performances from the box score of NFL games in order to automatically generate these summarizing game headlines. In particular, the paper explores various approaches consisting of neural networks and a variety of generative models to develop an effective system that is capable of completing the currently manual task. Some challenges faced by the system include summarizing a game from a largely naive data set that, in actuality, gives but a glimpse into the complete time-dependent game riddled with the intricacies common to sports. To help combat such issues, we simplified the problem statement to only involve the summarization of events in the actual game, without incorporating information that would require citation outside of the box score. As a result, injury reports, playoff contention, and game disputes are not incorporated into the headlines generated by the developed system.

\subsection{Motivation}
From sports to Wall Street, extrapolating key figures from a restricted set of data is an ongoing area of research with applications in a variety of fields. In addition, summarizing the identified key figures in a comprehensive headline may prove instrumental to fields in which language proves more powerful than numbers. As such, although this paper focuses on the information extraction and summarization of football games from box scores, the methods described in this paper may by applied to far reaching applications.


% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
\section{Related Work}
Generation of somewhat short sentences using Markov chains and stochastic methods have
been shown to be quite simple and effective in generating poetry \cite{1}\cite{2}, so we focused on applying these same principles in synthesizing valid headlines. This method yields a good way for text generation learned from examples; however, there is relatively much less research in the topic of generation solely from a combination of statistics and their corresponding text examples. Other, more syntactically-based models geared toward dialogue systems \cite{3}\cite{4}, we found to be inapplicable to our problem due to the lack of grammatical structure of the headlines in our data. Wen et. al. \cite{5} presented an empirically-tested LSTM model capable of generating linguistically varied responses, but applying a deep, recurrent model to our problem proved hard due to the lack of data and difficulty of integration of the statistics in training.

Football game summaries have been explored by Nichols et. al. \cite{6} and Chakrabarti \& Punera \cite{7}, but both methods relied on learning data directly from Twitter and were uninformed by the statistics of games, as our model was. 

\section{Data Collection}

The data used by the system was extracted directly from ESPN's publicly accessible website1. The website contains comprehensive historical statistics for the past ten seasons of NFL games (2007-2016), with a complete box score for each game played throughout each season. From these box score, we extracted twenty-eight key statistics that would provide substantial insight into each overall game. In addition to the statistics extracted directly from the box score, we also incorporated team specific information for each game, such as the teams' city and acronym. This information proved useful in the annotation of the game headlines described below.

In addition to the box score data, each game on the website also has an accompanying article headline that was manually written by a professional sports writer following the game. We scraped this headline for each of the games for use as training data for our system. However, as described in the next section, our approach took into consideration not the headline provided directly from the website, but a general form of the headline. To generalize the headlines, we removed all game specific names and figures and replaced them with identifying tags. A few examples of the general form headlines can be seen in listing \ref{annotation}. The initial process was completed automatically but required additional manual processing to ensure accurate annotation of the data. After the data was compiled and annotated, we removed any game data with an accompanying headline that incorporated information external to the actual gameplay, such as injury reports, playoff contention, and game disputes. In conclusion, our final dataset consisted of data and a headline from 1742 individual games.


\begin{listing}[H]
\begin{minted}[breaklines]{python}
"Kolb throws 2 TDs to lead Eagles' rout of Chiefs."

"[team_1_leader_passing] throws [team_1_leader_passing_td] TDs to lead [team_1_mascot] rout of [team_2_mascot]."
\end{minted}
\caption{An example of a training headline (top) and its corresponding annotation (bottom)}
\label{annotation}
\end{listing}


\section{Approach}

Our implementation is made up of two separate models, a neural net (Section IV.B) to map the size statistic vector $s$ into a word-likelihood vector $d$ and a text generator which we refer to as the Re-weighted Trigram Markov Generator, or RTMG (Section IV.C). An overview of the entire system is shown in figure \ref{model_graph}. 

\begin{figure*}[t]
\centering
\includegraphics[width=7in]{model_graph.png}
\caption{Box Diagram of the full headline generation system}
\label{model_graph}
\end{figure*}

\subsection{Statistic Feature Vectors}
We explored a variety of options for encoding the raw statistics into feature vectors. The first, a naive approach, was just the 18 numerical statistics from the box score data, and we found, as exprected, that it under-performed more complex representations. The second option was a concatenation of 1-hot encodings of the statistics to test a higher-dimensional separation of the data. This representation under-performed our optimal feature representation due to the sparsity of the feature vectors. We sampled various combinations of statistics using our priors on the structure of the data. After testing the representations at their optimal learning rate, we identified the features illustrated below. Some values were chosen unmodified, the others converted into a 1-hot classification vector to determine the numerical range in which the statistic lies. For example, we bucketed passing yards into 50 point ranges from $0-500+$ representing ranges $[0-50, 50-100,..., 500+]$. 

\begin{itemize}
  \item team\_1\_leader\_passing\_td  value
  \item team\_1\_leader\_rushing\_td value
  \item team\_1\_leader\_receiving\_td value
  \item team\_1\_leader\_passing\_yds $[0-50, 50-100,..., 500+]$
  \item team\_1\_leader\_rushing\_yds $[0-25, 25-50,..., 150+]$
  \item team\_1\_leader\_receiving\_yds $[0-25, 25-50,..., 150+]$
  \item game\_leader\_kicker\_points $[0-1, 1-2,..., 3+]$
  \item game\_leader\_scorer\_points $[0-1, 1-2,..., 3+]$
  \item team\_1\_leader\_passing\_int value
  \item team\_score\_diff $[0-2, 2-7,..., 21+]$
\end{itemize}

We found that statistics with magnitudes $>= 50$ should be cast as range classification vectors to decrease the dissimilarity of close numbers. Similarly, we did not cast low values $< 50$ as classification vectors but rather took their values. The majority of statistics focused on team 1, the winning team, as we found headlines generally were biased to the winning team. Furthermore, the optimal representation is comprehensive, covering offense, defense, and special teams of NFL football teams. 

\subsection{Neural Net} 
The NN accepts a vector, $s$, of statistics and returns a vector containing the probabilities of words existing in the headline associated with those statistics. First, the NN projects the statistics vector $s$ through two hidden layers of size $2|s|$. The hidden layer outputs are activated by a Rectified Linear function (ReLU). Then, the result is projected to a vector space of size $|V|$, where $V$ is the vocabulary including start and end characters. Finally, the output is activated by a softmax function. Refer to \ref{NN} for an illustration of the NN architecture. We initialize the weights by sampling from a Gaussian distribution $N(0,1)$. To train the model, we optimize a categorical cross-entropy loss function using Adam. This training method suits our objective of producing multi-class predictions. Lastly, we regularize our model on two fronts using: (1) Adam, a self-regularizing gradient descent method, and neural dropout, where we ignore the weights connected to neurons that are "turned off" with probability $p$ during training. With our vector $y$ of word probabilities $p(w_i)$ for $w_i \in V$, we move on to trigram MC.

\begin{figure*}[h!]
\centering
\includegraphics[width=.6\textwidth]{NN.png}
\caption{Overview of the neural network architecture. Input to the network is the feature vector $s$ and output is the word-likelihood vector $d$.}
\label{NN}
\end{figure*}

\subsection{Re-weighted Trigram Markov Generator}
We initialize the RTMG with transition probabilities estimated from the corpus of 1742 game headlines. For this task, we simply calculated the maximum likelihood estimates for each word as $p(w_i|w_{i-1},w_{i-2}) = \frac{count(w_i, w_{i-1}, w_{i-2})}{count(w_{i-1},w_{i-2})}$, yielding a matrix of transition probabilities, $T$. Refer to Figure 2 for an example illustration of the MC model. At this point, the MC could be used to create reasonably semantically and syntactically correct sentences. The model takes an element-wise product of our word probability vector $d$ from (1) and each row of $T$, which describe the transitions $u, v \rightarrow w$, for all words $w \in V$ given words $u, v$. These probabilities serve as discounting factors to properly weigh down the transition probabilities to words unlikely to appear in the headline. Leveraging this information results in generally grammatical sentences that are generated around the relevant vocabulary. Code for the re-weighting algorithm in showing in listing \ref{reweight_markov}. The function takes as input $T$, the existing transition matrix as estimated from the corpus of headlines. $T$ is re-weighted by $d$, which is the output of the neural net and has the same length as each row of $T$. The re-weighted transition matrix, $M$ is returned. Note that $d$ changes based on $s$, the input feature vector to the neural net, so $T$ is re-weighted only to generate sentences for a particular $s$ and then re-weighted differently for a different $s$.

Sentences are generated by doing a random walk over the possible states, where each word, including the start and end symbols, are considered states. The generator starts at a pair of two start symbols and then randomly walks over the words according to the transition probabilities until the end symbol is reached. If the current state is $(u,v)$, then the next state is chosen according to the probabilities in $M$ corresponding to pair $(u,v)$.

\begin{listing}[H]
\begin{minted}[breaklines]{python}
def reweight_markov(d, T):
  M = copy(T)
  for u in range(0, n*n):
    for v in range(0, n):
      M[u][v] = T[u][v] * d[v]
 return M
\end{minted}
\caption{Re-weighting the Markov text generator}
\label{reweight_markov}
\end{listing}


\section{Results}
We consider the neural network and the overall system (NN+MC) in our results and analysis. It is important to show that the NN learns the likelihood of words to appear in the headline, and then conveys this information to our MC. We partitioned the statistic and headline data into 70\% training (1,219 examples) and 30\% test data (523 examples). We trained the neural network until convergence, using the optimal batch size and number of epochs. For different numbers of hidden layers and neural dropout probability, we calculate the cross entropy loss. More over, for the T test examples, we calculate a quantity named similarity = $\sum_{t=1}^{T}$ Intersect(Top-K-Words(S$^{(i)}$),H$^{(i)}$) / $\sum_{t=1}^{T} H^{(i)}$.length, where Intersect calculates the intersection of Top-K-Words--the top k words by likelihood predicted by the neural network from the statistics features $S^{(i)}$ and the top k words in the test headline, $H^{(i)}$. The length property is simply the number of words in $H^{(i)}$. This metric measures the similarity of the k words in the test headline with the most likely k words according to our neural model. The loss and similarity results are reported in Table \ref{table_example}. 

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{The neural networks were trained with an initial learning rate of 0.05 using Adam.}
\label{table_example}
\centering
\begin{tabular}{|c||c||c||c|}
\hline
Num. of Hidden Layers & Dropout Probability & Loss & Similarity \\
\hline
1 & 0.10 & 3.76 & 0.42\\
2 & 0.10 & 3.62 & 0.45\\
3 & 0.10 & 3.47 & 0.49\\
1 & 0.25 & 3.69 & 0.44\\
2 & 0.25 & 3.61 & 0.45\\
3 & 0.25 & 3.41 & 0.54\\
1 & 0.50 & 3.87 & 0.35\\
2 & 0.50 & 3.67 & 0.41\\
3 & 0.50 & 3.52 & 0.45\\
\hline
\end{tabular}
\end{table}

The cross-entropy loss reflects the similarity of the predicted word-likelihood vectors and their input probabilities. As the number of hidden layers increases, the loss decreases across all dropout probabilities we tested. Despite the increasing complexity, our model does not degrade due to the increasing difficulty of optimization. The most performant model used three hidden layers and a dropout probability $p = 0.25$. The last batch of models experienced were over-regularized, with the the models failing to learn from the data optimally. 

Our system sought to map a feature vector of numerical statistics to a headline describing the game. We evaluated performance of the whole system by measuring the similarity of the headlines predicted from the test statistics and the T test headlines. This was found by first generating 30 sentences per test statistic vector with the MC. Then, we calculated the maximum similarity = max$_{i=1...30}$ Intersect($y^{(i)}$($S^{(j)}$), $H^{(j)}$) / $H^{(j)}$.length for i = 1...30, where $y^{(i)}(S^{(j)})$ represents the ith prediction on a fixed statistic vector and Intersect measures the intersection this and the associated test headline. The maximum similarities for all test data were then averaged. We take 30 measurements to reduce the inherent variance in the MC--a result of its random walk. We rejected all prediction headlines with a word count greater or less than five words from the test headline's count. It is important to note that this metric does not capture the predicted headline's similarity to other training headlines from which it learns its initial transition probability matrix. In fact, many of our longer, more descriptive predicted sentences appear to borrow from a number of training examples, despite being biased toward the words predicted by the neural networks. Results for the bigram and trigram markov models are reported in Table 2. 

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{We use the optimal neural model illustrated in Figure 2 to predict word likelihood vectors. We then re-weight (RW) the transition matrices of the bigram and trigram models.}
\label{table_example}
\centering
\begin{tabular}{|c||c||c|}
\hline
Model Type & Avg. Headline Word Length & Similarity \\
\hline
Bigram MC & 3.37 & 0.197\\
Trigram MC & 5.21 & 0.187\\
RW Bigram MC & 3.11 & 0.243\\
RW Trigram MC & 4.97 &  0.232\\

\hline
\end{tabular}
\end{table}

As expected, Trigram models tend to produce more complex sentences with lengths greater by nearly a factor of 2. Furthermore, the similarity of re-weighted models is greater. This is because by re-weighting the initial transition matrix, we bias the predicted headlines toward words included in the test headline we compare against. Since our neural network predicts these words with reasonable accuracy, we can conclude the system is capable of generating basic headlines from box score statistics.

\section{Contribution}

\subsection{Parker Greene}

I started by working on the model design, and after a lot of research into text generation, I initially came up with the idea of not using the raw statistics as input directly to the sentence generation model, but instead separating the system into the two separate models: one for creating an embedding from the statistics and one for using this embedding to generate the headlines. After the initial model design, I focused on the problem of text generation. The initial approach I considered was the bigram Markov chain, which had the ability to generate sentences, but could only learn directly from the corpus of headlines and thus relied on the neural net output. I also explored the possibility of using both word-level and text-level sequence generation using an LSTM network. This approach worked well for learning from the corpus how to generate sentences, but unlike the Markov model, it could not easily be integrated with the existing embeddings output by the neural net. I also explored the possibility of using PCFG's to generate the headlines, which would have been able to assure the headlines had a given structure, but, as was touched upon in section III, I found they could not properly work with the headlines we had, since the headlines did not follow proper grammar. We contributed equal work to this write-up and the presentation.

\subsection{Itamar Belson}
I started by researching existing work and models in the field of text generation as it pertained to our proposed problem. After compiling various ideas, we met as a group to discuss the benefits and the drawbacks of each approach. My main contribution to the project was the collection and annotation of the data. I developed a web scraper that would collect the historical data from ESPN?s website. This presented several challenges, as the data was not consistently formatted between seasons. After collecting all of the necessary data, my next task was to annotate the data by properly tagging each of the headlines and removing all referenced names and numbers. This process was done as a two-step process that consisted of automatic information extraction tagging and then manual assessment of each of the over two thousand headlines to ensure proper annotation of each data element. As a final step, I reviewed each of the headlines to identify any headline that referenced events external to the game, a key constraint of our system. After collecting and annotating the data, I helped the rest of the group in various ways. First, I worked with John to develop an ideal feature vector to optimally encapsulate the game statistics. This consisted of several iterations to ensure an optimized feature representation for the neural network. I also worked with the entire team to brainstorm system improvements and model specifications and helped solve several bugs along the way. Finally, I contributed equally to all team members in the final write-up and presentation.

\subsection{John Clarke}
I began by researching the design of neural networks used for multi-classification tasks. I iterated through numerous different neural network architectures (number of hidden layers, number of neurons, activation functions), evaluating them on the training data. I eventually implemented the optimal neural network used to transform statistic vectors into word likelihood vectors. I collaborated with Itamar on identifying the important features for our vector representation of the data. I worked with Parker to connect the neural network outputs to his markov models to generate headlines. After, we developed basic statistics to evaluate the success of our generative model where a specific metric was not obvious. We all collaborated on the write-up and presentation. 


\section{Source Code}

Code is available at \url{https://github.com/pwgreene/6.864-Final-Project}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.



% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

\begin{thebibliography}{9}
\bibitem{1} 
Kenner, Hugh; O'Rourke, Joseph (November 1984). ``A Travesty Generator for Micros". BYTE. 9 (12): 129-131, 449-469.
\newline
\bibitem{2}
 Hartman, Charles (1996). \textit{Virtual Muse: Experiments in Computer Poetry}. Hanover, NH: Wesleyan University Press.
\newline
\bibitem{3}
Alice  H. Oh and Alexander  I. Rudnicky. 2000. ``Stochastic language generation for spoken dialogue systems".  In Proceedings of the 2000 ANLP/NAACL Workshop  on Conversational  Systems  -  Volume  3, ANLP/NAACL-ConvSyst `00.
\newline
\bibitem{4}
Adwait Ratnaparkhi. 2002. ``Trainable approaches to surface natural language generation and their application to conversational dialog systems". Computer Speech and Language.
\newline
\bibitem{5}
Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola Mrksic,  Pei-Hao  Su,  David  Vandyke,  and  Steve Young.  2015.  ``Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking".  In Proceedings of SIG-dial. Association for Computational Linguistics.
\newline
\bibitem{6}
Jeffrey Nichols, Jalal Mahmud, Clemens Drews. 2012. ``Summarizing Sporting Events Using Twitter". IBM Research - Almaden
\newline
\bibitem{7}
Chakrabarti, D., and Punera, K. 2011. ``Event summarization using tweets". Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.

\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:


% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}

